---
title: "101c_mid"
author: "Anshuman Mahalley"
date: "11/6/2020"
output: html_document
---

```{r}
# Loading libraries
library(tidyverse)
library(haven)
library(caret)
library(MLeval)
library(mlbench)

# Importing training and test data
train_raw <- read_csv("training.csv")
test <- read_csv("test.csv")

# Converting the variable 'class' to a factor from numeric (Classification Problem)
train_raw$class <- as.factor(train_raw$class)
levels(train_raw$class) <- c('NG', 'OG', 'TSG')  # Renaming factor levels to the respective gene classes

train_raw <- na.omit(train_raw)

# Checking dimensions of training and test data
dim(train_raw) 
dim(test)

#Removing the first column 'id'
train_raw <- train_raw[,-1]

# EXPLORATATORY DATA ANALYSIS

glimpse(train_raw)  # Checking data type of all the variables in the dataset

# Creating a table to check class balance and the propotion of each class
summary(train_raw$class)
prop.table(table(train_raw$class))
```
```{r}
## Checking for outliers using Cook's distance
cooksd <- cooks.distance(glm(class ~ .,family = "binomial", data = train_raw))

# Plotting Cook's distance for each observation
plot(cooksd, 
     pch="*", 
     cex=2, 
     main="Influential Obs by Cooks distance")  
abline(h = 4*mean(cooksd, na.rm=T), col="red")

outliers <- rownames(train_raw[cooksd > 4*mean(cooksd, na.rm=T), ]) # Finding index of outlier points
print(outliers)


```


```{r}
#REmoving outliers
train_raw <- train_raw[-c(1:35),]

# Feature selection - Method 1: Removing highly correlated variables with correlation > 0.7
correlationMatrix <- cor(train_raw[,-98])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.70)
train1 <- train_raw[,-highlyCorrelated]
```


```{r}
## Setting seed to reproduce model
set.seed(123)

# Controlling parameters for our model - Bootstrapping samples from minority classes to handle class imbalance and using Cross-Validation 
train_control <- trainControl(method="cv", number = 5, classProbs = TRUE, savePredictions = "final", index = createResample(train1$class, 3),sampling = "up")


#LDA Model: Using PCA, Centering and Scaling to preprocess training dataset
LDAfit <- train(class~., data = train1, method = "lda", preProcess = c("pca", "center", "scale"), trControl= train_control)


#Evaluating model's performance and plotting ROC-AUC Curve 
res <- evalm(LDAfit)
res$roc

```

```{r}
# Code snippet to prepare data for Kaggle Submission

copy_test <- test # Creating a copy of test dataset
copy_test$class <- predict(LDAfit, newdata = copy_test)  #Assigining predicted outputs to 'class' column of copy_test dataframe

submission <- copy_test %>% dplyr:: select(id, class) # Subsetting copy_test dataframe to get relevant columns for submission
levels(submission$class) <- c(0,1,2) #Renaming factor levels to original values
colnames(submission) <- c("id", "class")
head(submission)
write.csv(submission, "m.csv", row.names = FALSE) # Saving file in .CSV format
```


